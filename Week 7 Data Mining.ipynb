{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b343acd9-4a1a-4380-8033-28fc5eae1842",
   "metadata": {},
   "source": [
    "## Project Milestone 3\n",
    "\n",
    "This project aims to build a robust model for the binary classification of email text, predicting if a message is spam (1) or ham (0). Milestone 3 is about transforming the $\\approx 83,446$ raw emails from the combined_data.csv file into numerical feature matrices using at least three engineering techniques to maximize the classification model's predictive power, directly addressing the advice that strong feature input is key to success in this milestone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e23c46-fe88-4dbb-9d0d-dbbdcad6de5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Dataset Loaded: combined_data.csv\n",
      "Shape: (83448, 2)\n",
      "\n",
      "First 5 Rows (for confirmation):\n",
      "                                                text  label\n",
      "0  ounce feather bowl hummingbird opec moment ala...      1\n",
      "1  wulvob get your medircations online qnb ikud v...      1\n",
      "2   computer connection from cnn com wednesday es...      0\n",
      "3  university degree obtain a prosperous future m...      1\n",
      "4  thanks for all your answers guys i know i shou...      0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chani\\AppData\\Local\\Temp\\ipykernel_20088\\3548291710.py:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['text'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Setup and Data Ingestion\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Import required libraries for cleaning and vectorization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# --- DATA INGESTION ---\n",
    "# CRITICAL: Loading the full dataset identified in Milestone 2.\n",
    "file_name = 'combined_data.csv'\n",
    "try:\n",
    "    # We use 'latin-1' or 'ISO-8859-1' encoding to handle text data that may contain special characters.\n",
    "    df = pd.read_csv(file_name, encoding='latin-1') \n",
    "    \n",
    "    # Generic column cleanup for common dataset structure\n",
    "    df.columns = ['label', 'text']\n",
    "    df = df[['text', 'label']].copy() # Keep only the two required columns\n",
    "\n",
    "    # Convert the label column to 0/1 (if not already)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    \n",
    "    # Handle any potential missing values by filling with an empty string\n",
    "    df['text'].fillna('', inplace=True)\n",
    "    \n",
    "    print(f\"Full Dataset Loaded: {file_name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 Rows (for confirmation):\")\n",
    "    print(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{file_name}' was not found. Please ensure it is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393b3cd-fb4e-441d-a79b-e4da85b2f524",
   "metadata": {},
   "source": [
    "### Feature Engineering 1\n",
    "\n",
    "Text Normalization is the fundamental first step, reducing noise and standardizing the corpus by applying lowercasing, punctuation removal, and stop word removal. Crucially, lemmatization simplifies the vocabulary by reducing words to their base form (e.g., \"running\" to \"run\"), ensuring the model generalizes better and improving overall efficiency. The resulting clean text column is ready for numerical vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eeb6b9d-caa6-4563-9747-0af5eb48a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Engineering 1: Text Normalization Results ---\n",
      "Original Text Sample: ounce feather bowl hummingbird opec moment alabaster valkyrie dyad bread flack desperate iambic hadron heft quell yoghurt bunkmate divert afterimage\n",
      "Cleaned Text Sample: ounce feather bowl hummingbird opec moment alabaster valkyrie dyad bread flack desperate iambic hadron heft quell yoghurt bunkmate divert afterimage\n"
     ]
    }
   ],
   "source": [
    "# Text Normalization Function and Application\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Ensure text is treated as string\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. Lowercasing\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. Remove punctuation, symbols, and non-alphanumeric characters (keeping only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) \n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 4. Stop Word Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and word]\n",
    "    \n",
    "    # 5. Lemmatization (Reducing to base form)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Rejoin tokens into a single clean string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the normalization function to the text column\n",
    "df['clean_text'] = df['text'].apply(normalize_text)\n",
    "\n",
    "print(\"--- Feature Engineering 1: Text Normalization Results ---\")\n",
    "print(f\"Original Text Sample: {df['text'].iloc[0]}\")\n",
    "print(f\"Cleaned Text Sample: {df['clean_text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439a996-c03a-4008-8c55-ba503b6c3036",
   "metadata": {},
   "source": [
    "### Feature Engineering 2\n",
    "\n",
    "The second step is TF-IDF (Term Frequency-Inverse Document Frequency) Vectorization, which is superior to simple word counts for spam classification. TF-IDF weights words based on their frequency in an email (TF) and their rarity across the corpus (IDF). This is critical for highlighting highly predictive spam terms (e.g., \"claim,\" \"urgent\"). We limit the resulting sparse matrix to 5000 features to ensure computational efficiency while retaining the most discriminative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b165b17-be7b-49be-8843-ac07ab1a77df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Engineering 2: TF-IDF Vectorization Results ---\n",
      "Shape of TF-IDF Matrix (Rows x Features): (83448, 5000)\n",
      "The number of unique features (words) retained is: 5000\n",
      "\n",
      "First 10 Features Retained by TF-IDF:\n",
      "['aa' 'ab' 'abbott' 'abc' 'ability' 'able' 'ableton' 'abroad' 'absence'\n",
      " 'absolute']\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "# Max features is set to 5000 to manage complexity and training time\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) \n",
    "\n",
    "# Fit the vectorizer to the clean text and transform the data\n",
    "# The resulting matrix (X) is the input feature set for the classification model\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Get the feature names (the words/tokens)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"--- Feature Engineering 2: TF-IDF Vectorization Results ---\")\n",
    "print(f\"Shape of TF-IDF Matrix (Rows x Features): {tfidf_matrix.shape}\")\n",
    "print(f\"The number of unique features (words) retained is: {tfidf_matrix.shape[1]}\")\n",
    "print(\"\\nFirst 10 Features Retained by TF-IDF:\")\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc07e81-be94-4ca7-9669-025772accdc7",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering 3\n",
    "\n",
    "The final step uses Bag of N-Grams (Bigrams) to introduce context by capturing sequences of two consecutive words (e.g., \"click here\"). This is crucial because phrases are often far more indicative of spam than individual words alone. By creating a 5000-feature matrix of Bigrams, we ensure the model learns the syntax of spam messages, significantly improving predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c1bb53-9545-4263-8f15-a26ab0cf1db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature Engineering 3: Bag of N-Grams (Bigrams) Results ---\n",
      "Shape of N-Gram Matrix (Rows x Bigram Features): (83448, 5000)\n",
      "The number of unique Bigram features is: 5000\n",
      "\n",
      "First 10 N-Gram Feature Names (Bigrams):\n",
      "['aac escapenumber' 'able find' 'able get' 'able work' 'ableton live'\n",
      " 'ac check' 'ac cv' 'ac msg' 'ac subst' 'ac uk']\n"
     ]
    }
   ],
   "source": [
    "# Bag of N-Grams (Bigrams) Vectorization\n",
    "\n",
    "# Initialize the Count Vectorizer for N-Grams\n",
    "# ngram_range=(2, 2) specifies that we ONLY want Bigrams (pairs of words).\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2), max_features=5000)\n",
    "\n",
    "# Fit and transform the clean text\n",
    "ngram_matrix = ngram_vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Get the feature names (the Bigrams)\n",
    "ngram_feature_names = ngram_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"--- Feature Engineering 3: Bag of N-Grams (Bigrams) Results ---\")\n",
    "print(f\"Shape of N-Gram Matrix (Rows x Bigram Features): {ngram_matrix.shape}\")\n",
    "print(f\"The number of unique Bigram features is: {ngram_matrix.shape[1]}\")\n",
    "print(\"\\nFirst 10 N-Gram Feature Names (Bigrams):\")\n",
    "print(ngram_feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3438572-4867-4df0-80fa-ba6ceafa92eb",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "I have created three essential feature sets for our model:\n",
    "1.  Clean Text (for human analysis)\n",
    "2.  TF-IDF Matrix (single-word importance)\n",
    "3.  N-Gram Matrix (contextual phrase importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
